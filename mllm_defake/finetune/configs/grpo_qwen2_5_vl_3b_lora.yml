# model
model_name_or_path: "/home/fanqi/projects/mllm-defake-kun/output/Qwen2.5-VL-3B-SFT-LoRA/checkpoint-150-merged"
torch_dtype: "bfloat16"
freeze_vision: true
min_pixels: 3136
max_pixels: 12845056
# data
dataset_name: "data/processed_v0/conversations.jsonl"
images_root: ""
val_split_ratio: 0
data_seed: 6737151
# reward
reward_version: "v0"
# template
max_prompt_length: 8192
num_generations: 4
# trainer
output_dir: "output/Qwen2.5-VL-3B-GRPO-LoRA"
deepspeed: "zero2"
per_device_train_batch_size: 1
learning_rate: 1e-5
num_train_epochs: 3
gradient_accumulation_steps: 16
gradient_checkpointing: false
save_steps: 100
save_total_limit: 5
warmup_ratio: 0.05
dataloader_num_workers: 4
report_to: "tensorboard"
logging_steps: 1
# peft
use_peft: true
lora_r: 8
lora_alpha: 32
lora_task_type: "CAUSAL_LM"
